---
title: JAX
---

# [JAX](https://github.com/google/jax)

[Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/), [JAX in Action](https://www.manning.com/books/jax-in-action) & [JaxTon](https://github.com/vopani/jaxton) are great. [Meta Optimal Transport](https://github.com/facebookresearch/meta-ot) is nice JAX repo to run/study.

[Robert Lange](https://github.com/RobertTLange) has nice JAX repos.

Learning to use [BlackJAX](https://github.com/blackjax-devs/blackjax) to do Bayesian inference on models.

## Notes

- [JAX is more like NumPy with Autograd, Jit, and TPU/GPU support](https://twitter.com/rasbt/status/1538903583365681152)

## Links

- [audax](https://github.com/SarthakYadav/audax) - Home for audio ML in JAX. Has common features, learnable frontends, pretrained supervised and self-supervised models.
- [tinygp](https://github.com/dfm/tinygp) - Extremely lightweight library for building Gaussian Process models in Python, built on top of jax.
- [GPJax](https://github.com/thomaspinder/GPJax) - Didactic Gaussian process package for researchers in Jax.
- [Mctx](https://github.com/deepmind/mctx) - Monte Carlo tree search in JAX.
- [Pipelined Swarm Training](https://github.com/kingoflolz/swarm-jax) - Swarm training framework using Haiku + JAX + Ray for layer parallel transformer language models on unreliable, heterogeneous nodes.
- [JAX MuZero](https://github.com/Hwhitetooth/jax_muzero) - JAX implementation of the MuZero agent.
- [Jax Influence](https://github.com/google-research/jax-influence) - Scalable implementation of Influence Functions in JaX.
- [BlackJAX](https://github.com/blackjax-devs/blackjax) - Library of samplers for JAX that works on CPU as well as GPU. ([Twitter](https://twitter.com/blackjax_mcmc)) ([Contribute](https://github.com/blackjax-devs/blackjax/issues/154)) ([Sampling Book](https://blackjax-devs.github.io/sampling-book/)) ([Sampling Book Code](https://github.com/blackjax-devs/sampling-book))
- [GPax](https://github.com/google-research/gpax) - Jax/Flax codebase for Gaussian processes including meta and multi-task Gaussian processes.
- [jax-fenics-adjoint](https://github.com/IvanYashchuk/jax-fenics-adjoint) - Differentiable interface to FEniCS/Firedrake for JAX using dolfin-adjoint/pyadjoint.
- [jax-ekf](https://github.com/brentyi/jax-ekf) - Generic EKF, with support for non-Euclidean manifolds.
- [PaLM - Jax](https://github.com/lucidrains/PaLM-jax) - Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways - in Jax.
- [Pre-trained image classification models for Jax/Haiku](https://github.com/abarcel/haikumodels)
- [Flaxformer: transformer architectures in JAX/Flax](https://github.com/google/flaxformer)
- [KFAC-JAX - Second Order Optimization with Approximate Curvature in JAX](https://github.com/deepmind/kfac-jax)
- [flowjax](https://github.com/danielward27/flowjax) - Normalizing flow implementations in jax.
- [Jax3D](https://github.com/google-research/jax3d) - Library for neural rendering in Jax and aims to be a nimble NeRF ecosystem.
- [DALL·E 2 in JAX](https://github.com/lucidrains/DALLE2-jax)
- [JAXNS](https://github.com/Joshuaalbert/jaxns) - Nested sampling in JAX.
- [AUX](https://github.com/deepmind/dm_aux) - Audio processing library in JAX, for JAX.
- [Nice DeepMind Jax libraries](https://twitter.com/DeepMind/status/1517146462571794433)
- [Machine Learning with JAX - From Zero to Hero (2021)](https://www.youtube.com/playlist?list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
- [Flax](https://github.com/google/flax) - Neural network library for JAX designed for flexibility. ([Docs](https://flax.readthedocs.io/en/latest/))
- [JAX talks by HuggingFace](https://www.youtube.com/playlist?list=PLo2EIpI_JMQtQrEduYXbRz4X50mTiOi8S)
- [Homomorphic Encryption in JAX](https://github.com/nkandpa2/he_jax)
- [JAX implementation of Learning to learn by gradient descent by gradient descent](https://github.com/teddykoker/learning-to-learn-jax)
- [Normalizing Flows in JAX](https://github.com/ChrisWaites/jax-flows)
- [Big Vision](https://github.com/google-research/big_vision) - Designed for training large-scale vision models on Cloud TPU VMs. Based on Jax/Flax libraries.
- [Jax vs. Julia (Vs PyTorch) (2022)](https://kidger.site/thoughts/jax-vs-julia/) ([HN](https://news.ycombinator.com/item?id=31263516))
- [minGPT in JAX](https://github.com/mgrankin/minGPT)
- [flaxvision](https://github.com/rolandgvc/flaxvision) - Selection of neural network models ported from torchvision for JAX & Flax.
- [JAX version of clip guided diffusion scripts](https://github.com/nshepperd/jax-guided-diffusion)
- [Functorch](https://github.com/pytorch/functorch) - Jax-like composable function transforms for PyTorch. ([HN](https://news.ycombinator.com/item?id=31424588))
- [Ninjax](https://github.com/danijar/ninjax) - Module system for JAX that offers full state access and allows to easily combine modules from other libraries.
- [Functional Transformer](https://github.com/awf/functional-transformer) - Pure-functional implementation of a machine learning transformer model in Python/JAX.
- [JAX + Units](https://github.com/dfm/jpu) - Provides and interface between JAX and Pint to allow JAX to support operations with units.
- [Infinite Recommendation Networks (∞-AE) in JAX](https://github.com/noveens/infinite_ae_cf)
- [Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/) ([Code](https://github.com/ericmjl/dl-workshop))
- [Algorithms for Privacy-Preserving Machine Learning in JAX](https://github.com/deepmind/jax_privacy)
- [Connex](https://github.com/leonard-gleyzer/connex) - Small JAX library built on Equinox whose aim is to incorporate artificial analogues of biological neural network attributes into deep learning research and architecture design.
- [Rax](https://github.com/google/rax) - Composable Learning to Rank using JAX.
- [JaX is faster than PyTorch but harder to debug](https://twitter.com/kevin_zakka/status/1538634474107314176)
- [JAX Meta Learning](https://github.com/tristandeleu/jax-meta-learning) - Collection of meta-learning algorithms in JAX.
- [Gymnax](https://github.com/RobertTLange/gymnax) - RL Environments in JAX.
- [Pax](https://github.com/google/paxml) - Framework to configure and run machine learning experiments on top of Jax.
- [SymPy2Jax](https://github.com/google/sympy2jax) - Turn SymPy expressions into trainable JAX expressions.
- [JAX Typing](https://github.com/google/jaxtyping) - Type annotations and runtime checking for shape and dtype of JAX arrays, and PyTrees.
- [CoDeX](https://github.com/google/codex) - Data compression in JAX.
- [DiBS](https://github.com/larslorch/dibs) - Python JAX implementation for DiBS, fully differentiable method for joint Bayesian inference of the DAG and parameters of general, causal Bayesian networks.
- [Generative Adversarial Networks in JAX](https://github.com/lweitkamp/GANs-JAX)
- [Neural implicit queries](https://github.com/nmwsharp/neural-implicit-queries) - Perform geometric queries on neural implicit surfaces like ray casting, intersection testing, fast mesh extraction, closest points, and more.
- [CLIP-JAX](https://github.com/borisdayma/clip-jax) - Train CLIP models using JAX and transformers.
- [BLOOM Inference in JAX](https://github.com/huggingface/bloom-jax-inference)
- [v-diffusion-jax](https://github.com/crowsonkb/v-diffusion-jax) - V objective diffusion inference code for JAX.
- [Euclidean Neural Networks Jax](https://github.com/e3nn/e3nn-jax)
- [Jax + CUDA boilerplate](https://github.com/brentyi/jax_cuda_boilerplate)
- [mlff](https://github.com/thorben-frank/mlff) - Build neural networks for machine learning force fields with JAX.
- [Jax-Triton](https://github.com/jax-ml/jax-triton) - Integrations between JAX and Triton.
- [JAX Synergistic Memory Inspector](https://github.com/ayaka14732/jax-smi) - Tool for real-time inspection of the memory usage of a JAX process.
- [Jaxformer](https://github.com/salesforce/jaxformer) - Minimal library to train LLMs on TPU in JAX.
- [xpag](https://github.com/perrin-isir/xpag) - Modular reinforcement learning library with JAX agents.
- [Orbax](https://github.com/google/orbax) - Library providing common utilities for JAX users.
- [Praxis](https://github.com/google/praxis) - Layer library for Pax. While Praxis is optimized for ML at scale, Praxis has a goal to be usable by other JAX-based ML projects.
- [JAX in Action (2022)](https://www.manning.com/books/jax-in-action)
- [DeepMind JAX Ecosystem](https://github.com/deepmind/jax)
- [JAX Tutorial](https://github.com/pierreglaser/jax-tutorial)
- [SBX: Stable Baselines Jax (SB3 + Jax)](https://github.com/araffin/sbx)
- [Ciclo](https://github.com/cgarciae/ciclo) - Training loop utilities and abstractions for JAX.
- [DYNAMAX](https://github.com/probml/dynamax) - State Space Models library in JAX.
- [Myriad](https://github.com/nikihowe/myriad) - Real-world testbed that aims to bridge trajectory optimization and deep learning.
- [JaQMC](https://github.com/bytedance/jaqmc) - JAX accelerated Quantum Monte Carlo.
- [Rieoptax](https://github.com/SaitejaUtpala/rieoptax) - Riemannian Optimization Using JAX.
- [Jax 0.4.0](https://twitter.com/cgarciae88/status/1601315046503567360)
- [JAXGA](https://github.com/RobinKa/jaxga) - JAX Geometric Algebra. ([HN](https://news.ycombinator.com/item?id=33948008))
- [PyTorch Lightning + Jax](https://github.com/ludwigwinkler/JaxLightning)
- [Structural Time Series (STS) in JAX](https://github.com/probml/sts-jax)
- [safejax](https://github.com/alvarobartt/safejax) - Serialize JAX/Flax models with safetensors.
- [dejax](https://github.com/hr0nix/dejax) - Accelerated replay buffers in JAX.
- [JaxTon](https://github.com/vopani/jaxton) - 100 exercises to learn JAX.
- [JMP](https://github.com/deepmind/jmp) - Mixed Precision library for JAX.
- [PIPs JAX](https://github.com/brentyi/pips-jax) - JAX implementation of Persistent Independent Particles.
- [Jax Decompiler](https://github.com/PierrickPochelu/JaxDecompiler)
- [MACE](https://github.com/ACEsuit/mace-jax) - Equivariant machine learning interatomic potentials in JAX.
- [JAX Wavelets](https://github.com/crowsonkb/jax-wavelets) - 2D discrete wavelet transform for JAX.
- [Autodidax: JAX core from scratch](https://jax.readthedocs.io/en/latest/autodidax.html) ([HN](https://news.ycombinator.com/item?id=34732361))
- [Aesara on JAX](https://github.com/rlouf/aejax)
- [Training Deep Networks with Data Parallelism in Jax (2023)](https://www.mishalaskin.com/posts/data_parallel) ([HN](https://news.ycombinator.com/item?id=34926778))
- [JAX – Augments numpy and Python code with function transformations (2019)](https://colinraffel.com/blog/you-don-t-know-jax.html) ([HN](https://news.ycombinator.com/item?id=34979201))
- [PureJaxRL](https://github.com/luchris429/purejaxrl) - End-to-End RL Training in Pure Jax.
- [4000x Speedup in Reinforcement Learning with Jax (2023)](https://chrislu.page/blog/meta-disco/) ([HN](https://news.ycombinator.com/item?id=35474968))
- [Creative Machine Learning](https://github.com/acids-ircam/creative_ml) - Creative Machine Learning course and notebooks in JAX, PyTorch and Numpy.
- [LAST](https://github.com/google-research/last) - JAX library for building lattice-based speech transducer models.
- [MaxText](https://github.com/google/maxtext) - Simple, performant and scalable Jax LLM.
- [Stable Diffusion JAX](https://github.com/patil-suraj/stable-diffusion-jax)
- [JAX-AM](https://github.com/tianjuxue/jax-am) - Additive manufacturing simulation with JAX.
- [AutoBound](https://github.com/google/autobound) - Automatically computes upper and lower bounds on functions.
- [WAX-ML](https://github.com/eserie/wax-ml) - Python library for machine-learning and feedback loops on streaming data.
- [Oryx](https://github.com/jax-ml/oryx) - Library for probabilistic programming and deep learning built on top of Jax.
- [Path Tracing in JAX](https://github.com/ericjang/pt-jax)
- [JAX Metrics](https://github.com/cgarciae/jax_metrics) - Metrics library for the JAX ecosystem.
- [Lineax](https://github.com/google/lineax) - Linear solvers in JAX and Equinox.
- [Levanter and Haliax](https://github.com/stanford-crfm/levanter) - Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax.
